(cm_pruned[1,1] + cm_pruned[2,2]) / nrow(oj_test)
cm_pruned = table(oj_pred_pruned, oj_test[,'Purchase'])
cm_pruned
(cm_pruned[1,1] + cm_pruned[2,2]) / nrow(oj_test)
cm
cm_pruned
(cm[1,1] + cm[2,2]) / nrow(oj_test)
(cm_pruned[1,1] + cm_pruned[2,2]) / nrow(oj_test)
set.seed(0)
cv_oj = cv.tree(tree_oj, FUN = prune.misclass)
#6. Visualize your results from part 5 across various numbers of terminal nodes/values for alpha.
names(cv_oj)
cv_oj
par(mfrow = c(1, 2))
plot(cv_oj$size, cv_oj$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv_oj$k, cv_oj$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
#7. Prune your tree based on the results of part 6.
#Pruning the tree to have 22 terminal nodes.
prune_oj = prune.tree(tree_oj, best = 22)
#8. How many terminal nodes are there in your pruned tree? What is the accuracy of your pruned tree?
summary(prune_oj)
# Classification tree:
#   snip.tree(tree = tree_oj, nodes = c(54L, 145L, 289L, 11L, 40L,
#                                       26L, 73L, 288L, 66L, 41L, 34L, 65L, 67L, 12L, 7L, 35L))
# Variables actually used in tree construction:
#   [1] "SpecialMM"      "SpecialCH"      "DiscCH"         "DiscMM"         "LoyalCH"        "PriceMM"
# [7] "PriceDiff"      "STORE"          "WeekofPurchase" "PctDiscMM"
# Number of terminal nodes:  23
# Residual mean deviance:  0.7709 = 656 / 851
# Misclassification error rate: 0.1625 = 142 / 874
#=> number of terminal nodes: 23
#=> Accuracy: 1 - 0.1625 = 0.8375
#9. Visualize your pruned tree.
par(mfrow = c(1, 1))
plot(prune_oj)
text(prune_oj, pretty = 0)
#10. Predict the  Purchase variable for observations that are within your test set using this pruned tree. Report the accuracy of your predictions.
oj_pred_pruned = predict(prune_oj, oj_test, type = "class")
cm_pruned = table(oj_pred_pruned, oj_test[,'Purchase'])
cm_pruned
# oj_pred_pruned  CH  MM
# CH 104  27
# MM  20  63
(cm_pruned[1,1] + cm_pruned[2,2]) / nrow(oj_test)
set.seed(0)
length = nrow(OJ)
test_rows = sample(1:length, 0.2 * length, replace=TRUE)
oj_test = OJ[test_rows,]
oj_training = OJ[-test_rows,]
# 2. Construct an initial decision tree predicting  Purchase from all other variables
# in the training dataset defining splits based upon the Gini coefficient.
library(tree)
tree_oj = tree(Purchase ~ . - Purchase, split = "gini", data = OJ, subset = oj_training)
# 3. How many terminal nodes are there in your initial tree? What is the accuracy of your initial tree?
summary(tree_oj)
# Classification tree:
#   tree(formula = Purchase ~ . - Purchase, data = oj_training, split = "gini")
# Variables actually used in tree construction:
#   [1] "SpecialMM"      "SpecialCH"      "DiscCH"         "DiscMM"         "LoyalCH"        "WeekofPurchase"
# [7] "STORE"          "PriceDiff"      "PriceCH"        "StoreID"        "PriceMM"        "SalePriceMM"
# [13] "PctDiscMM"      "ListPriceDiff"
# Number of terminal nodes:  95
# Residual mean deviance:  0.6101 = 475.3 / 779
# Misclassification error rate: 0.1396 = 122 / 874
#=> Number of terminal nodes: 95. Accuracy: 1- 0.1396 = 0.8604
#4. Predict the  Purchase variable for observations that are within your test set using this initial tree.
#Report the accuracy of your predictions.
oj_pred = predict(tree_oj, oj_test, type = "class")
oj_pred
summary(oj_pred)
# Confusion Matrix
cm = table(oj_pred, oj_test[,'Purchase'])
cm
# oj_pred CH MM
# CH 99 26
# MM 25 64
(cm[1,1] + cm[2,2]) / nrow(oj_test)
# 0.7616822
#=> accuracy: 0.7616822
#5. Implement cross-validation and, thus, cost-complexity pruning to determine how far back to
#prune your tree. (NB: Use  set.seed(0)so your results will be reproducible.)
#Performing cross-validation in order to decide how many splits to prune; using
#misclassification as the basis for pruning.
set.seed(0)
cv_oj = cv.tree(tree_oj, FUN = prune.misclass)
#6. Visualize your results from part 5 across various numbers of terminal nodes/values for alpha.
names(cv_oj)
cv_oj
par(mfrow = c(1, 2))
plot(cv_oj$size, cv_oj$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv_oj$k, cv_oj$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
#7. Prune your tree based on the results of part 6.
#Pruning the tree to have 22 terminal nodes.
prune_oj = prune.tree(tree_oj, best = 22)
#8. How many terminal nodes are there in your pruned tree? What is the accuracy of your pruned tree?
summary(prune_oj)
# Classification tree:
#   snip.tree(tree = tree_oj, nodes = c(54L, 145L, 289L, 11L, 40L,
#                                       26L, 73L, 288L, 66L, 41L, 34L, 65L, 67L, 12L, 7L, 35L))
# Variables actually used in tree construction:
#   [1] "SpecialMM"      "SpecialCH"      "DiscCH"         "DiscMM"         "LoyalCH"        "PriceMM"
# [7] "PriceDiff"      "STORE"          "WeekofPurchase" "PctDiscMM"
# Number of terminal nodes:  23
# Residual mean deviance:  0.7709 = 656 / 851
# Misclassification error rate: 0.1625 = 142 / 874
#=> number of terminal nodes: 23
#=> Accuracy: 1 - 0.1625 = 0.8375
#9. Visualize your pruned tree.
par(mfrow = c(1, 1))
plot(prune_oj)
text(prune_oj, pretty = 0)
#10. Predict the  Purchase variable for observations that are within your test set using this pruned tree. Report the accuracy of your predictions.
oj_pred_pruned = predict(prune_oj, oj_test, type = "class")
cm_pruned = table(oj_pred_pruned, oj_test[,'Purchase'])
cm_pruned
# oj_pred_pruned  CH  MM
# CH 104  27
# MM  20  63
(cm_pruned[1,1] + cm_pruned[2,2]) / nrow(oj_test)
# 0.7803738
library(tree)
#Loading the ISLR library in order to use the Carseats dataset.
#install.packages('ISLR')
library(ISLR)
#Making data manipulation easier.
help(Carseats)
attach(Carseats)
#Looking at the variable of interest, Sales.
hist(Sales)
summary(Sales)
#Creating a binary categorical variable High based on the continuous Sales
#variable and adding it to the original data frame.
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
#Fit a tree to the data; note that we are excluding Sales from the formula.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
# rResidual mean deviance is actually "Gini", prefer smaller Gini
#The output shows the variables actually used within the tree, the number of
#terminal nodes, the residual mean deviance based on the Gini index, and
#the misclassification error rate.
#Plotting the classification tree.
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
#Detailed information for the splits of the classification tree.
tree.carseats
#The output shows the variables used at each node, the split rule, the number
#of observations at each node, the deviance based on the Gini index, the
#majority class value based on the observations in the node, and the associated
#probabilities of class membership at each node. Terminal nodes are denoted
#by asterisks.
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
#Ftting and visualizing a classification tree to the training data.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
#Using the trained decision tree to classify the test data.
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
#Assessing the accuracy of the overall tree by constructing a confusion matrix.
table(tree.pred, High.test)
(60 + 42)/120
#Performing cross-validation in order to decide how many splits to prune; using
#misclassification as the basis for pruning.
set.seed(0)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
#Inspecting the elements of the cv.tree() object.
names(cv.carseats)
cv.carseats
#Size indicates the number of terminal nodes. Deviance is the criterion we
#specify; in this case it is the misclassification rate. K is analogous to the
#cost complexity tuning parameter alpha. Method indicates the specified criterion.
#Visually inspecting the results of the cross-validation by considering tree
#complexity.
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.carseats$k, cv.carseats$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
#Pruning the overall tree to have 4 terminal nodes; choose the best tree with
#4 terminal nodes based on cost complexity pruning.
par(mfrow = c(1, 1))
prune.carseats = prune.misclass(tree.carseats, best = 4)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
#Assessing the accuracy of the pruned tree with 4 terminal nodes by constructing
#a confusion matrix.
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(53 + 33)/120
library(tree)
#Loading the ISLR library in order to use the Carseats dataset.
#install.packages('ISLR')
library(ISLR)
#Making data manipulation easier.
help(Carseats)
attach(Carseats)
#Looking at the variable of interest, Sales.
hist(Sales)
summary(Sales)
#Creating a binary categorical variable High based on the continuous Sales
#variable and adding it to the original data frame.
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
#Fit a tree to the data; note that we are excluding Sales from the formula.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
# rResidual mean deviance is actually "Gini", prefer smaller Gini
#The output shows the variables actually used within the tree, the number of
#terminal nodes, the residual mean deviance based on the Gini index, and
#the misclassification error rate.
#Plotting the classification tree.
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
#Detailed information for the splits of the classification tree.
tree.carseats
#The output shows the variables used at each node, the split rule, the number
#of observations at each node, the deviance based on the Gini index, the
#majority class value based on the observations in the node, and the associated
#probabilities of class membership at each node. Terminal nodes are denoted
#by asterisks.
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
#Ftting and visualizing a classification tree to the training data.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
#Using the trained decision tree to classify the test data.
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
#Assessing the accuracy of the overall tree by constructing a confusion matrix.
table(tree.pred, High.test)
(60 + 42)/120
#Performing cross-validation in order to decide how many splits to prune; using
#misclassification as the basis for pruning.
set.seed(0)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
#Inspecting the elements of the cv.tree() object.
names(cv.carseats)
cv.carseats
#Size indicates the number of terminal nodes. Deviance is the criterion we
#specify; in this case it is the misclassification rate. K is analogous to the
#cost complexity tuning parameter alpha. Method indicates the specified criterion.
#Visually inspecting the results of the cross-validation by considering tree
#complexity.
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.carseats$k, cv.carseats$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
#Pruning the overall tree to have 4 terminal nodes; choose the best tree with
#4 terminal nodes based on cost complexity pruning.
par(mfrow = c(1, 1))
prune.carseats = prune.misclass(tree.carseats, best = 4)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
#Assessing the accuracy of the pruned tree with 4 terminal nodes by constructing
#a confusion matrix.
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(53 + 33)/120
library(ISLR)
data(OJ)
head(OJ)
str(OJ)
set.seed(0)
length = nrow(OJ)
test_rows = sample(1:length, 0.2 * length, replace=TRUE)
oj_test = OJ[test_rows,]
oj_training = OJ[-test_rows,]
# 2. Construct an initial decision tree predicting  Purchase from all other variables
library(randomForest)
help(Boston)
library(MASS)
help(Boston)
library(randomForest)
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training, importance = TRUE)
library(ISLR)
data(OJ)
head(OJ)
str(OJ)
set.seed(0)
length = nrow(OJ)
test_rows = sample(1:length, 0.2 * length, replace=TRUE)
oj_test = OJ[test_rows,]
oj_training = OJ[-test_rows,]
# 2. Construct an initial decision tree predicting  Purchase from all other variables
# in the training dataset defining splits based upon the Gini coefficient.
library(tree)
tree_oj = tree(Purchase ~ . - Purchase, split = "gini", data = OJ, subset = oj_training)
# 3. How many terminal nodes are there in your initial tree? What is the accuracy of your initial tree?
summary(tree_oj)
# Classification tree:
#   tree(formula = Purchase ~ . - Purchase, data = oj_training, split = "gini")
# Variables actually used in tree construction:
#   [1] "SpecialMM"      "SpecialCH"      "DiscCH"         "DiscMM"         "LoyalCH"        "WeekofPurchase"
library(randomForest)
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training, importance = TRUE)
library(randomForest)
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training, importance = TRUE)
library(randomForest)
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training, importance = TRUE)
library(randomForest)
#Fitting an initial random forest to the training subset.
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training, importance = TRUE)
rf_oj = randomForest(Purchase ~ . - Purchase, data = OJ, subset = oj_training, importance = TRUE)
OJ
head(OJ)
head(oj_training)
rf_oj = randomForest(Purchase ~ . - Purchase, data = OJ, subset = oj_training, importance = TRUE)
rf_oj = randomForest(Purchase ~ . - Purchase, data = OJ, subset = oj_training, importance = TRUE)
rf_oj = randomForest(Purchase ~ . - Purchase, data = OJ, subset = oj_training, importance = TRUE)
?ranodomForest
?randomForest
library(randomForest)
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training, importance = TRUE)
library(randomForest)
#Fitting an initial random forest to the training subset.
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
head(OJ)
head(oj_training)
library(randomForest)
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training, importance = TRUE)
help(Boston)
help(Boston)
library(MASS)
help(Boston)
?prune.misclass
?gbm
library(gbm)    #not just for trees
?gbm
library(ISLR)
data(OJ)
head(OJ)
str(OJ)
set.seed(0)
length = nrow(OJ)
oj_training_rows = sample(1:length, 0.8 * length)
oj_training = OJ[oj_training_rows,]
oj_test = OJ[-oj_training_rows,]
nrow(oj_test)
# 2. Construct an initial decision tree predicting  Purchase from all other variables
# in the training dataset defining splits based upon the Gini coefficient.
library(tree)
tree_oj = tree(Purchase ~ ., split = "gini", data = OJ, subset = oj_training_rows)
# 3. How many terminal nodes are there in your initial tree? What is the accuracy of your initial tree?
summary(tree_oj)
# Classification tree:
#   tree(formula = Purchase ~ ., data = OJ, subset = oj_training_rows,
#        split = "gini")
# Variables actually used in tree construction:
#   [1] "SpecialMM"      "SpecialCH"      "DiscCH"         "DiscMM"         "LoyalCH"        "STORE"
# [7] "WeekofPurchase" "PriceMM"        "StoreID"        "PriceCH"        "PriceDiff"      "Store7"
# [13] "SalePriceMM"    "ListPriceDiff"  "PctDiscMM"
# Number of terminal nodes:  86
# Residual mean deviance:  0.6051911 = 465.9971 / 770
oj_pred = predict(tree_oj, oj_test, type = "class")
oj_pred
summary(oj_pred)
# Confusion Matrix
cm = table(oj_pred, oj_test[,'Purchase'])
cm
# oj_pred  CH  MM
# CH 106  24
# MM  25  59
(cm[1,1] + cm[2,2]) / nrow(oj_test)
# 0.7710280374
#=> accuracy: 0.7710280374
#5. Implement cross-validation and, thus, cost-complexity pruning to determine how far back to
#prune your tree. (NB: Use  set.seed(0)so your results will be reproducible.)
#Performing cross-validation in order to decide how many splits to prune; using
#misclassification as the basis for pruning.
set.seed(0)
cv_oj = cv.tree(tree_oj, FUN = prune.misclass)
names(cv_oj)
cv_oj
par(mfrow = c(1, 2))
plot(cv_oj$size, cv_oj$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv_oj$k, cv_oj$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
best.nodes = cv_oj$size[which(cv_oj$dev == min(cv_oj$dev))]
best.nodes = cv_oj$size[which(cv_oj$dev == min(cv_oj$dev))]
best.nodes
#=> best tree has 11 nodes
#Pruning the tree to have 11 terminal nodes.
prune_oj = prune.tree(tree_oj, best = best.nodes)
# should use this one?
#prune_oj = prune.misclass(tree_oj, best = best.nodes)
#8. How many terminal nodes are there in your pruned tree? What is the accuracy of your pruned tree?
summary(prune_oj)
summary(prune_oj)
par(mfrow = c(1, 1))
plot(prune_oj)
text(prune_oj, pretty = 0)
oj_pred_pruned = predict(prune_oj, oj_test, type = "class")
cm_pruned = table(oj_pred_pruned, oj_test[,'Purchase'])
cm_pruned
(113+57)/nrow(oj_test)
library(randomForest)
set.seed(0)
rf_oj = randomForest(Purchase ~ ., data = OJ, subset = oj_training_rows, importance = TRUE)
rf_oj
varImpPlot(rf_oj)
set.seed(0)
num_predictors = ncol(OJ) - 1
oob.err = numeric(num_predictors)
for (mtry in 1:num_predictors) {
fit = randomForest(Purchase ~ ., data = OJ, subset = oj_training_rows, mtry = mtry)
oob.err[mtry] = fit$err.rate[500,1]
cat("We're performing iteration", mtry, "\n")
}
plot(1:num_predictors, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
plot(1:num_predictors, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
oob.err
oob.err
1- min(oob.err)
1- oob.err[-1]
1- oob.err[17]
1- oob.err[num_predictors]
set.seed(0)
fit = randomForest(Purchase ~ ., data = OJ, subset = oj_training_rows, mtry = 2)
rf_pred_test = predict(fit, oj_test, type = "class")
accuracy = confusion(rf_pred_test, oj_test[,'Purchase'])
confusion <- function(pred, test){
tbl = table(pred, test)
accu = sum(diag(tbl)) / sum(tbl)
list(table = tbl, accuracy = accu)
}
accuracy = confusion(rf_pred_test, oj_test[,'Purchase'])$accuracy
accuracy
set.seed(0)
fit = randomForest(Purchase ~ ., data = OJ, subset = oj_training_rows, mtry = 17)
rf_pred_test = predict(fit, oj_test, type = "class")
accuracy = confusion(rf_pred_test, oj_test[,'Purchase'])$accuracy
accuracy
shiny::runApp('Documents/DataScience/projects/web_scraping/web_sraping_app')
install.packages('devtools')
devtools::install_github('rstudio/rsconnect')
rsconnect::setAccountInfo(name='blin02', token='8C3642C99C1DAC7B529A32C387DE3C4F', secret='9eXJ7wIu0dakaELriZfN2DI9fhOYfB/Gg0ycN2LD')
library(rsconnect)
rsconnect::deployApp('/Users/binlin/Documents/DataScience/projects/web_scraping/web_sraping_app/')
rsconnect::deployApp('/Users/binlin/Documents/DataScience/projects/web_scraping/online_news_popularity/')
shiny::runApp('Documents/DataScience/projects/web_scraping/online_news_popularity')
shiny::runApp('Documents/DataScience/projects/web_scraping/online_news_popularity')
shiny::runApp('Documents/DataScience/projects/web_scraping/online_news_popularity')
shiny::runApp('Documents/DataScience/projects/Shiny Project/food_price_app')
rsconnect::deployApp('/Users/binlin/Documents/DataScience/projects/Shiny Project/food_price_changes/')
shiny::runApp('Documents/DataScience/projects/Shiny Project/food_price_changes')
shiny::runApp('Documents/DataScience/projects/Shiny Project/food_price_changes')
shiny::runApp('Documents/DataScience/projects/Shiny Project/food_price_changes')
rsconnect::deployApp('/Users/binlin/Documents/DataScience/projects/Shiny Project/food_price_changes/')
setwd("/Users/binlin/Documents/DataScience/projects/Shiny Project/food_price_changes/data/")
producer.price = read.csv("producer/producer_price.csv", na.strings = "N/A")
producer.price.transpose = setNames(data.frame(t(producer.price[,c(-1, -2)])), producer.price[,2])
producer.price.transpose$year  = substr(rownames(producer.price.transpose), 2, 5)
producer.price.transpose.dim()
dim(producer.price.transpose)
is.na(producer.price.transpose)
colSums(is.na(producer.price.transpose))
category_percent = read.csv("./data/category_percent.csv")
setwd("/Users/binlin/Documents/DataScience/projects/Shiny Project/food_price_changes/data/")
category_percent = read.csv("./data/category_percent.csv")
category_percent = read.csv("./category_percent.csv")
category_percent
?sort
sort(category_percent, Percent)
category_percent[order(Percent),]
category_percent[order(Percent),]
category_percent[order(category_percent$Percent),]
category_percent[order(-category_percent$Percent),]
category_percent = filter(category_percent, !(category %in% c('All.food', 'Food.at.home', 'Meats')))
category_percent[order(-category_percent$Percent),]
category_percent = filter(category_percent, !(category %in% c('All.food', 'Food.at.home', 'Meats')))
category_percent[order(-category_percent$Percent),]
shiny::runApp('~/Documents/DataScience/projects/Shiny Project/food_price_changes')
category_percent = read.csv("./category_percent.csv")
category_percent = filter(category_percent, !(category %in% c('All.food', 'Food.at.home', 'Meats')))
category_percent[order(-category_percent$Percent),]
rsconnect::deployApp('/Users/binlin/Documents/DataScience/projects/Shiny Project/food_price_changes/')
